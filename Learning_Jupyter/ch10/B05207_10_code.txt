#first spark script
import pyspark
if not 'sc' in globals():
    sc = pyspark.SparkContext()

lines = sc.textFile("Spark File Words.ipynb")
lineLengths = lines.map(lambda s: len(s))
totalLength = lineLengths.reduce(lambda a, b: a + b) 
print(totalLength)
---------------------------------------------------------
#spark word count
import pyspark
if not 'sc' in globals():
    sc = pyspark.SparkContext()

text_file = sc.textFile("Spark File Words.ipynb")
counts = text_file.flatMap(lambda line: line.split(" ")) \
             .map(lambda word: (word, 1)) \
             .reduceByKey(lambda a, b: a + b)
for x in counts.collect():
    print x
---------------------------------------------------------
#sorted word count
import pyspark
if not 'sc' in globals():
    sc = pyspark.SparkContext()
    
text_file = sc.textFile("Spark File Words.ipynb")
sorted_counts = text_file.flatMap(lambda line: line.split(" ")) \
            .map(lambda word: (word, 1)) \
            .reduceByKey(lambda a, b: a + b) \
            .sortByKey()
for x in sorted_counts.collect():
    print x
---------------------------------------------------------
#estimate pi
import pyspark
import random

if not 'sc' in globals():
    sc = pyspark.SparkContext()

NUM_SAMPLES = 1000

def sample(p):
    x,y = random.random(),random.random()
    return 1 if x*x + y*y < 1 else 0

count = sc.parallelize(xrange(0, NUM_SAMPLES)) \
            .map(sample) \
            .reduce(lambda a, b: a + b)
        
print "Pi is roughly %f" % (4.0 * count / NUM_SAMPLES)
---------------------------------------------------------
#log file examination
import pyspark
if not 'sc' in globals():
    sc = pyspark.SparkContext()

textFile = sc.textFile("access_log")
print(textFile.count(),"access records")

gets = textFile.filter(lambda line: "GET" in line)
print(gets.count(),"GETs")

posts = textFile.filter(lambda line: "POST" in line)
print(posts.count(),"POSTs")

other = textFile.subtract(gets).subtract(posts)
print(other.count(),"Other")
for x in other.collect():
    print x
---------------------------------------------------------
#spark primes
import pyspark
if not 'sc' in globals():
    sc = pyspark.SparkContext()
    
def is_it_prime(number):
    # make sure n is a positive integer
    number = abs(int(number))
    
    # simple tests
    if number < 2:
        return False
    
    # 2 is prime
    if number == 2:
        return True
    # other even numbers aren't
    if not number & 1:
        return False
    
    # check whether number is divisible into it's square root
    for x in range(3, int(number**0.5)+1, 2):
        if number % x == 0:
            return False
        
    #if we get this far we are good
    return True

# create a set of numbers to 100,000
numbers = sc.parallelize(xrange(100000))

# count out the number of primes we found
print numbers.filter(is_it_prime).count()
---------------------------------------------------------
#spark text file analysis
import pyspark
if not 'sc' in globals():
    sc = pyspark.SparkContext()
     
sentences = sc.textFile('2600raid.txt') \
    .glom() \
    .map(lambda x: " ".join(x)) \
    .flatMap(lambda x: x.split("."))
print(sentences.count(),"sentences")

bigrams = sentences.map(lambda x:x.split()) \
    .flatMap(lambda x: [((x[i],x[i+1]),1) for i in range(0,len(x)-1)])
print(bigrams.count(),"bigrams")

frequent_bigrams = bigrams.reduceByKey(lambda x,y:x+y) \
    .map(lambda x:(x[1],x[0])) \
    .sortByKey(False)
frequent_bigrams.take(10)
---------------------------------------------------------
#spark evaluating history data
import pyspark
import csv
import operator
import itertools
import collections

if not 'sc' in globals():
    sc = pyspark.SparkContext()
               
years = {}
occupations = {}
guests = {}

#The file header contains these column descriptors
#YEAR,GoogleKnowlege_Occupation,Show,Group,Raw_Guest_List

with open('daily_show_guests.csv', 'rb') as csvfile:    
    reader = csv.DictReader(csvfile)
    for row in reader:
        year = row['YEAR']
        if years.has_key(year):
            years[year] = years[year] + 1
        else:
            years[year] = 1
        
        occupation = row['GoogleKnowlege_Occupation']
        if occupations.has_key(occupation):
            occupations[occupation] = occupations[occupation] + 1
        else:
            occupations[occupation] = 1
            
        guest = row['Raw_Guest_List']
        if guests.has_key(guest):
            guests[guest] = guests[guest] + 1
        else:
            guests[guest] = 1

syears = sorted(years.items(), key=operator.itemgetter(1), reverse=True)
soccupations = sorted(occupations.items(), key=operator.itemgetter(1), reverse=True)
sguests = sorted(guests.items(), key=operator.itemgetter(1), reverse=True)

print syears[:5]
print soccupations[:5]
print sguests[:5]

